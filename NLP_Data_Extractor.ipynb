{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wget | grep -v 'already satisfied' #(added section to remove clutter of dependancies alredy installed)\n",
        "!pip install PyPDF2 | grep -v 'already satisfied'\n",
        "!pip install textract | grep -v 'already satisfied'\n",
        "!pip install PyMuPDF | grep -v 'already satisfied'"
      ],
      "metadata": {
        "id": "S4ppuXuXsrgt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6becc4ea-71e8-4924-ca56-3c1114e48b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py): started\n",
            "  Building wheel for wget (setup.py): finished with status 'done'\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=402a05023b0268f15981075c0c368187da7948d7aec708e12b8c4725355c5b23\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.6/232.6 kB 1.5 MB/s eta 0:00:00\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting textract\n",
            "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
            "Collecting argcomplete~=1.10.0 (from textract)\n",
            "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting beautifulsoup4~=4.8.0 (from textract)\n",
            "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.9/106.9 kB 2.2 MB/s eta 0:00:00\n",
            "Collecting chardet==3.* (from textract)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.4/133.4 kB 7.5 MB/s eta 0:00:00\n",
            "Collecting docx2txt~=0.8 (from textract)\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting extract-msg<=0.29.* (from textract)\n",
            "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.0/69.0 kB 8.1 MB/s eta 0:00:00\n",
            "Collecting pdfminer.six==20191110 (from textract)\n",
            "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 31.3 MB/s eta 0:00:00\n",
            "Collecting python-pptx~=0.6.18 (from textract)\n",
            "  Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 471.6/471.6 kB 37.0 MB/s eta 0:00:00\n",
            "Collecting six~=1.12.0 (from textract)\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting SpeechRecognition~=3.8.1 (from textract)\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32.8/32.8 MB 42.6 MB/s eta 0:00:00\n",
            "Collecting xlrd~=1.2.0 (from textract)\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.3/103.3 kB 14.7 MB/s eta 0:00:00\n",
            "Collecting pycryptodome (from pdfminer.six==20191110->textract)\n",
            "  Downloading pycryptodome-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 62.8 MB/s eta 0:00:00\n",
            "Collecting imapclient==2.1.0 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74.0/74.0 kB 9.1 MB/s eta 0:00:00\n",
            "Collecting olefile>=0.46 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.6/114.6 kB 13.4 MB/s eta 0:00:00\n",
            "Collecting compressed-rtf>=1.0.6 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting ebcdic>=1.1.1 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.5/128.5 kB 15.8 MB/s eta 0:00:00\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx~=0.6.18->textract)\n",
            "  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.8/154.8 kB 20.0 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: docx2txt, compressed-rtf\n",
            "  Building wheel for docx2txt (setup.py): started\n",
            "  Building wheel for docx2txt (setup.py): finished with status 'done'\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=5923326bbda339e11f356ddce42bb87e44db035930ad5354fe129ce0b2bbf2ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "  Building wheel for compressed-rtf (setup.py): started\n",
            "  Building wheel for compressed-rtf (setup.py): finished with status 'done'\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6184 sha256=7742b8fa72c4fbf1a44c615636ded215232cffef4e884cba24567314c46d3218\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/3e/48/e7d833ecc516c36f8966d310b1a6386db091a718f1ff3bf85c\n",
            "Successfully built docx2txt compressed-rtf\n",
            "Installing collected packages: SpeechRecognition, ebcdic, docx2txt, compressed-rtf, chardet, argcomplete, XlsxWriter, xlrd, six, pycryptodome, olefile, beautifulsoup4, python-pptx, pdfminer.six, imapclient, extract-msg, textract\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 2.0.1\n",
            "    Uninstalling xlrd-2.0.1:\n",
            "      Successfully uninstalled xlrd-2.0.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.11.2\n",
            "    Uninstalling beautifulsoup4-4.11.2:\n",
            "      Successfully uninstalled beautifulsoup4-4.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "pydrive2 1.6.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "yfinance 0.2.32 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SpeechRecognition-3.8.1 XlsxWriter-3.1.9 argcomplete-1.10.3 beautifulsoup4-4.8.2 chardet-3.0.4 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.47 pdfminer.six-20191110 pycryptodome-3.19.0 python-pptx-0.6.23 six-1.12.0 textract-1.6.5 xlrd-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.23.7-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 21.1 MB/s eta 0:00:00\n",
            "Collecting PyMuPDFb==1.23.7 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 30.6/30.6 MB 33.6 MB/s eta 0:00:00\n",
            "Installing collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.23.7 PyMuPDFb-1.23.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kphyjx4_zVmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a35c44-a24c-4f67-c3f0-7d67211e3ce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import bs4\n",
        "import wget\n",
        "import fitz\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "import textract\n",
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import urllib.request\n",
        "from urllib.request import urlopen\n",
        "from urllib.request import urlparse\n",
        "from ctypes.util import find_library\n",
        "import csv\n",
        "from google.colab import drive\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kFHlbTS3zVmx"
      },
      "outputs": [],
      "source": [
        "#extracting the titel and page number\n",
        "def titel(file):\n",
        "    reader = PdfReader(file)\n",
        "    meta = reader.metadata\n",
        "    #print(len(reader.pages))\n",
        "    #print(meta.title)\n",
        "    return len(reader.pages)\n",
        "#titel(file)\n",
        "\n",
        "#extract the text from the pdf\n",
        "def text(file):\n",
        "    pdfReader = PyPDF2.PdfReader(file)\n",
        "    num_pages = len(pdfReader.pages)\n",
        "\n",
        "    count = 0\n",
        "    text = \"\"\n",
        "\n",
        "    while count < num_pages:\n",
        "        #error pageObj = pdfReader.getPage(count)\n",
        "        pageObj = pdfReader.pages[count]\n",
        "        count +=1\n",
        "        text += pageObj.extract_text()\n",
        "\n",
        "    if text != \"\":\n",
        "        text = text\n",
        "\n",
        "    else:\n",
        "        text = textract.process('words.txt', method='tesseract', language='eng')\n",
        "\n",
        "\n",
        "    text = text.encode('ascii','ignore').lower()\n",
        "    text_decoded = text.decode()\n",
        "    return text_decoded\n",
        "\n",
        "#Pagewise Implimentation\n",
        "\n",
        "\n",
        "def extract_page(file,num=0):\n",
        "    pdf_file = fitz.open(file)\n",
        "    page_texts = []\n",
        "\n",
        "    for page_number in range(len(pdf_file)):\n",
        "        page = pdf_file.load_page(page_number)\n",
        "        page_text = page.get_text()\n",
        "\n",
        "        # Encode the page text as ASCII and convert to lowercase\n",
        "        page_text = page_text.encode('ascii', 'ignore').lower()\n",
        "        page_text_decoded = page_text.decode()\n",
        "        page_texts.append(page_text_decoded)\n",
        "\n",
        "    #page=int(input(\"Enter Page Number: \"))\n",
        "    filepage = \"\"\n",
        "\n",
        "    if page_texts:\n",
        "      filepage = page_texts[num]\n",
        "\n",
        "\n",
        "    return filepage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0-P7O2vszVmy"
      },
      "outputs": [],
      "source": [
        "def cleanText(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = text.replace(\"/\", \" \")\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"''\", \"\")\n",
        "    text = text.replace(\"  \", \" \")\n",
        "    text = text.replace(\"  \", \" \")\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub('[‘’“”…]', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    return text\n",
        "round1 = lambda x: cleanText(x)\n",
        "\n",
        "#Remove stopwords\n",
        "stopwords_en = stopwords.words('english')\n",
        "additionalStopWords = [\"pg\",\"figure\",\"u\"]    #Remove the words that are present in the title\n",
        "stopwords_en.extend(additionalStopWords)\n",
        "def toggleStopword(newStopword):\n",
        "  if newStopword in stopwords_en:\n",
        "        stopwords_en.remove(newStopword)\n",
        "  else:\n",
        "        stopwords_en.append(newStopword)\n",
        "\n",
        "def removeStopwords(texts):\n",
        "    extract = ''\n",
        "    for word in simple_preprocess(texts):\n",
        "        if word not in stopwords_en:\n",
        "            extract = extract + ' ' + word\n",
        "    return extract\n",
        "\n",
        "#Lemmatize keywords\n",
        "def lemmatizeKeywords(keywords):\n",
        "    keywords_list = keywords.split(' ')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    keywords_lemmatized = []\n",
        "    for keyword in keywords_list:\n",
        "        keywords_lemmatized.append(lemmatizer.lemmatize(keyword))\n",
        "    return ' '.join(keywords_lemmatized)\n",
        "\n",
        "\n",
        "def identifyFrequentKeywords(keywordsFlat):\n",
        "    keywords = keywordsFlat.split(' ')\n",
        "    while '' in keywords:\n",
        "        keywords.remove('')\n",
        "    keywordsFlat = ' '.join(keywords)\n",
        "    keywordCount = len(keywords)\n",
        "\n",
        "    trigrams_keys = []\n",
        "    trigrams_values = []\n",
        "    trigrams = nltk.trigrams(keywords)\n",
        "    frequency = nltk.FreqDist(trigrams)\n",
        "    for key,value in frequency.items():\n",
        "        if value > 1: #value->absolute count\n",
        "            trigrams_keys.append(' '.join(key))\n",
        "            trigrams_values.append(value/keywordCount) #->persisting relative frequency\n",
        "\n",
        "    bigrams_keys = []\n",
        "    bigrams_values = []\n",
        "    bigrams = nltk.bigrams(keywordsFlat.split(' '))\n",
        "    frequency = nltk.FreqDist(bigrams)\n",
        "    for key,value in frequency.items():\n",
        "        if value > 1:\n",
        "            bigrams_keys.append(' '.join(key))\n",
        "            bigrams_values.append(value/keywordCount)\n",
        "\n",
        "    monograms_keys = []\n",
        "    monograms_values = []\n",
        "    frequency = nltk.FreqDist(keywordsFlat.split(' '))\n",
        "    for key,value in frequency.items():\n",
        "        if value > 1:\n",
        "            monograms_keys.append(key)\n",
        "            monograms_values.append(value/keywordCount)\n",
        "\n",
        "    keys_selected = trigrams_keys + bigrams_keys + monograms_keys\n",
        "    keys_selected_values = trigrams_values + bigrams_values + monograms_values\n",
        "    keys_selected = [x for _, x in sorted(zip(keys_selected_values, keys_selected), reverse=True)]\n",
        "    keys_selected_values.sort(reverse=True)\n",
        "\n",
        "    #select top-n keywords\n",
        "    numberOfTopWords = 5\n",
        "    if len(keys_selected) > numberOfTopWords:\n",
        "        keys_selected        = keys_selected[:numberOfTopWords]\n",
        "        keys_selected_values = keys_selected_values[:numberOfTopWords]\n",
        "\n",
        "    return keys_selected, keys_selected_values, keywordCount\n",
        "\n",
        "#mostFrequentKeywords, relativeKeywordFrequency = identifyFrequentKeywords(lemmatizeKeywords(removeStopwords(round1(text(file)))))\n",
        "#print(mostFrequentKeywords)\n",
        "#print(relativeKeywordFrequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fdcp_ysIzVm1"
      },
      "outputs": [],
      "source": [
        "def word_count(word, text1):\n",
        "    with open('/content/drive/MyDrive/test6.txt','w+') as f:\n",
        "        #f.write(cleanText(text(file1)))\n",
        "        f.write(text1)\n",
        "    with open('/content/drive/MyDrive/test6.txt') as f:\n",
        "        return ''.join(f).count(word)\n",
        "\n",
        "\n",
        "#print(\"Number of words in the file :\", word_count(input('give me a word')))\n",
        "\n",
        "def word_cooccurrence(word1,word2,file):\n",
        "  page_list=[]\n",
        "  pages=titel(file)\n",
        "  for i in range(pages):\n",
        "    n=extract_page(file,i)\n",
        "    n=n.lower()\n",
        "    if word1.lower() in n:\n",
        "      if word2.lower() in n:\n",
        "        page_list.append(i+1)\n",
        "  return page_list\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def CSVFreq(file):\n",
        "  pages=titel(file)\n",
        "  CSV=[]\n",
        "  for i in range(pages):\n",
        "      n=extract_page(file,i)\n",
        "      x=n\n",
        "      x=cleanText(x)\n",
        "      x=lemmatizeKeywords(x)                                  #Turn words into their root ( Happily to Happy, faster to fast, etc)\n",
        "      x=removeStopwords(x)                                    #Only to be used if there's any specific words we want to remove\n",
        "      m=identifyFrequentKeywords(x)                           #Produces two element list of lists\n",
        "      a,b,c=m                                                 #alloting each list to a seperate variable\n",
        "      print(i+1)\n",
        "      print(m)\n",
        "      for j in range(len(a)):\n",
        "          CSV.append([i+1,a[j],b[j],c])\n",
        "  return CSV"
      ],
      "metadata": {
        "id": "ogTf0NxCRLbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QkEF6mr-zVmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b19346d1-5d47-493c-8371-3bc10adc672b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "EU\n"
          ]
        }
      ],
      "source": [
        "def SetCountry(country):#Write Country name, dont forget capitals first letter\n",
        "  drive.mount('/content/drive')\n",
        "  file = open('/content/drive/MyDrive/Sparsh_Policies/'+country+'.pdf','rb')\n",
        "  return file,country;\n",
        "\n",
        "file,country=SetCountry(\"EU\")\n",
        "print(country)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=word_cooccurrence(\"EU\",\"Hydrogen\",file)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ2fuZtEet5V",
        "outputId": "eec2c6ac-0bf2-4ad3-ec28-9c0f8e4b36ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 15, 18, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(CSVFreq(file))       #TODO: Add automatic removal of words in title, create local graphs for data if possible.\n",
        "print(titel(file))\n",
        "\n",
        "\n",
        "x=CSVFreq(file)\n",
        "with open('/content/drive/MyDrive/Sparsh_Policies/'+country+'_Frequency.csv','w+',newline='') as f:\n",
        "     writer = csv.writer(f)\n",
        "     writer.writerows(x)\n",
        "\n",
        "toggleStopword(country)\n",
        "toggleStopword('hydrogen') # Add Hydrogen to stopwords\n",
        "\n",
        "x=CSVFreq(file)\n",
        "with open('/content/drive/MyDrive/Sparsh_Policies/'+country+'_Frequency(without_hydrogen).csv','w+',newline='') as f:\n",
        "     writer = csv.writer(f)\n",
        "     writer.writerows(x)\n",
        "\n",
        "toggleStopword('hydrogen')  # Remove Hydrogen from stopwords again for next file\n",
        "toggleStopword(country)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOXlToleSzxn",
        "outputId": "6925636b-c8db-4c88-a2bb-04a049a10a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n",
            "1\n",
            "(['european', 'final', 'en', 'council', 'committee'], [0.18181818181818182, 0.09090909090909091, 0.09090909090909091, 0.09090909090909091, 0.09090909090909091], 22)\n",
            "2\n",
            "(['energy', 'supply', 'russian', 'russia', 'repowereu'], [0.06976744186046512, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372], 172)\n",
            "3\n",
            "(['energy', 'eu', 'state', 'member state', 'member'], [0.034055727554179564, 0.030959752321981424, 0.021671826625386997, 0.021671826625386997, 0.021671826625386997], 323)\n",
            "4\n",
            "(['energy', 'saving', 'efficiency', 'measure', 'eu'], [0.09121621621621621, 0.030405405405405407, 0.02702702702702703, 0.02027027027027027, 0.016891891891891893], 296)\n",
            "5\n",
            "(['energy', 'gas', 'demand', 'commission', 'state'], [0.03832752613240418, 0.020905923344947737, 0.017421602787456445, 0.017421602787456445, 0.013937282229965157], 287)\n",
            "6\n",
            "(['eu', 'platform', 'energy', 'supply', 'state'], [0.03296703296703297, 0.03021978021978022, 0.03021978021978022, 0.013736263736263736, 0.013736263736263736], 364)\n",
            "7\n",
            "(['solar', 'energy', 'wind', 'heat', 'commission'], [0.040160642570281124, 0.040160642570281124, 0.024096385542168676, 0.020080321285140562, 0.020080321285140562], 249)\n",
            "8\n",
            "(['hydrogen', 'renewable', 'renewable hydrogen', 'million', 'infrastructure'], [0.09090909090909091, 0.03162055335968379, 0.023715415019762844, 0.019762845849802372, 0.019762845849802372], 253)\n",
            "9\n",
            "(['biomethane', 'eu', 'production', 'gas', 'sustainable'], [0.03557312252964427, 0.02766798418972332, 0.023715415019762844, 0.023715415019762844, 0.019762845849802372], 253)\n",
            "10\n",
            "(['renewable', 'hydrogen', 'energy', 'industrial', 'repowereu'], [0.02768166089965398, 0.02422145328719723, 0.02422145328719723, 0.020761245674740483, 0.01730103806228374], 289)\n",
            "11\n",
            "(['energy', 'skill', 'eu', 'pump', 'material'], [0.023809523809523808, 0.02040816326530612, 0.02040816326530612, 0.017006802721088437, 0.017006802721088437], 294)\n",
            "12\n",
            "(['renewable', 'energy', 'state', 'member state', 'member'], [0.039285714285714285, 0.039285714285714285, 0.03214285714285714, 0.03214285714285714, 0.03214285714285714], 280)\n",
            "13\n",
            "(['gas', 'energy', 'year', 'project', 'bcm'], [0.03987730061349693, 0.03067484662576687, 0.024539877300613498, 0.02147239263803681, 0.018404907975460124], 326)\n",
            "14\n",
            "(['gas', 'supply', 'infrastructure', 'project', 'pipeline'], [0.03636363636363636, 0.024242424242424242, 0.021212121212121213, 0.01818181818181818, 0.015151515151515152], 330)\n",
            "15\n",
            "(['energy', 'electricity', 'oil', 'system', 'supply'], [0.03508771929824561, 0.03070175438596491, 0.021929824561403508, 0.017543859649122806, 0.017543859649122806], 228)\n",
            "16\n",
            "([], [], 4)\n",
            "17\n",
            "(['state', 'member state', 'member', 'investment', 'commission'], [0.03355704697986577, 0.03355704697986577, 0.03355704697986577, 0.026845637583892617, 0.026845637583892617], 149)\n",
            "18\n",
            "(['state', 'member state', 'member', 'energy', 'transfer'], [0.02821316614420063, 0.02821316614420063, 0.02821316614420063, 0.025078369905956112, 0.0219435736677116], 319)\n",
            "19\n",
            "(['state', 'project', 'member state', 'member', 'energy'], [0.04833836858006042, 0.027190332326283987, 0.027190332326283987, 0.027190332326283987, 0.027190332326283987], 331)\n",
            "20\n",
            "(['commission', 'state', 'member state', 'member', 'energy'], [0.02214022140221402, 0.01845018450184502, 0.01845018450184502, 0.01845018450184502, 0.01845018450184502], 271)\n",
            "21\n",
            "(['europe', 'energy', 'russia', 'reduce', 'plan'], [0.03225806451612903, 0.026881720430107527, 0.021505376344086023, 0.021505376344086023, 0.021505376344086023], 186)\n",
            "1\n",
            "(['european', 'final', 'en', 'council', 'committee'], [0.18181818181818182, 0.09090909090909091, 0.09090909090909091, 0.09090909090909091, 0.09090909090909091], 22)\n",
            "2\n",
            "(['energy', 'supply', 'russian', 'russia', 'repowereu'], [0.06976744186046512, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372], 172)\n",
            "3\n",
            "(['energy', 'eu', 'state', 'member state', 'member'], [0.03426791277258567, 0.03115264797507788, 0.021806853582554516, 0.021806853582554516, 0.021806853582554516], 321)\n",
            "4\n",
            "(['energy', 'saving', 'efficiency', 'measure', 'eu'], [0.09152542372881356, 0.030508474576271188, 0.02711864406779661, 0.020338983050847456, 0.01694915254237288], 295)\n",
            "5\n",
            "(['energy', 'gas', 'demand', 'commission', 'state'], [0.03859649122807018, 0.021052631578947368, 0.017543859649122806, 0.017543859649122806, 0.014035087719298246], 285)\n",
            "6\n",
            "(['eu', 'platform', 'energy', 'supply', 'state'], [0.03333333333333333, 0.030555555555555555, 0.030555555555555555, 0.013888888888888888, 0.013888888888888888], 360)\n",
            "7\n",
            "(['solar', 'energy', 'wind', 'heat', 'commission'], [0.04032258064516129, 0.04032258064516129, 0.024193548387096774, 0.020161290322580645, 0.020161290322580645], 248)\n",
            "8\n",
            "(['renewable', 'million', 'infrastructure', 'industry', 'tonne'], [0.034782608695652174, 0.021739130434782608, 0.021739130434782608, 0.021739130434782608, 0.017391304347826087], 230)\n",
            "9\n",
            "(['biomethane', 'eu', 'production', 'gas', 'sustainable'], [0.036, 0.028, 0.024, 0.024, 0.02], 250)\n",
            "10\n",
            "(['renewable', 'energy', 'industrial', 'repowereu', 'production'], [0.028368794326241134, 0.024822695035460994, 0.02127659574468085, 0.01773049645390071, 0.01773049645390071], 282)\n",
            "11\n",
            "(['energy', 'skill', 'eu', 'pump', 'material'], [0.023972602739726026, 0.02054794520547945, 0.02054794520547945, 0.017123287671232876, 0.017123287671232876], 292)\n",
            "12\n",
            "(['renewable', 'energy', 'state', 'member state', 'member'], [0.039285714285714285, 0.039285714285714285, 0.03214285714285714, 0.03214285714285714, 0.03214285714285714], 280)\n",
            "13\n",
            "(['gas', 'energy', 'year', 'project', 'bcm'], [0.03987730061349693, 0.03067484662576687, 0.024539877300613498, 0.02147239263803681, 0.018404907975460124], 326)\n",
            "14\n",
            "(['gas', 'supply', 'infrastructure', 'project', 'pipeline'], [0.03669724770642202, 0.024464831804281346, 0.021406727828746176, 0.01834862385321101, 0.01529051987767584], 327)\n",
            "15\n",
            "(['energy', 'electricity', 'oil', 'system', 'supply'], [0.035398230088495575, 0.030973451327433628, 0.022123893805309734, 0.017699115044247787, 0.017699115044247787], 226)\n",
            "16\n",
            "([], [], 4)\n",
            "17\n",
            "(['state', 'member state', 'member', 'investment', 'commission'], [0.03355704697986577, 0.03355704697986577, 0.03355704697986577, 0.026845637583892617, 0.026845637583892617], 149)\n",
            "18\n",
            "(['state', 'member state', 'member', 'energy', 'transfer'], [0.02830188679245283, 0.02830188679245283, 0.02830188679245283, 0.025157232704402517, 0.0220125786163522], 318)\n",
            "19\n",
            "(['state', 'project', 'member state', 'member', 'energy'], [0.04833836858006042, 0.027190332326283987, 0.027190332326283987, 0.027190332326283987, 0.027190332326283987], 331)\n",
            "20\n",
            "(['commission', 'state', 'member state', 'member', 'energy'], [0.022222222222222223, 0.018518518518518517, 0.018518518518518517, 0.018518518518518517, 0.018518518518518517], 270)\n",
            "21\n",
            "(['europe', 'energy', 'russia', 'reduce', 'plan'], [0.03225806451612903, 0.026881720430107527, 0.021505376344086023, 0.021505376344086023, 0.021505376344086023], 186)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}