{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install wget | grep -v 'already satisfied' #(added section to remove clutter of dependancies alredy installed)\n",
        "!pip install PyPDF2 | grep -v 'already satisfied'\n",
        "!pip install textract | grep -v 'already satisfied'\n",
        "!pip install PyMuPDF | grep -v 'already satisfied'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import bs4\n",
        "import wget\n",
        "import fitz\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "import textract\n",
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import urllib.request\n",
        "from urllib.request import urlopen\n",
        "from urllib.request import urlparse\n",
        "from ctypes.util import find_library\n",
        "import csv\n",
        "from google.colab import drive\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#extracting the titel and page number\n",
        "def titel(file):\n",
        "    reader = PdfReader(file)\n",
        "    meta = reader.metadata\n",
        "    #print(len(reader.pages))\n",
        "    #print(meta.title)\n",
        "    return len(reader.pages)\n",
        "#titel(file)\n",
        "\n",
        "#extract the text from the pdf\n",
        "def text(file):\n",
        "    pdfReader = PyPDF2.PdfReader(file)\n",
        "    num_pages = len(pdfReader.pages)\n",
        "\n",
        "    count = 0\n",
        "    text = \"\"\n",
        "\n",
        "    while count < num_pages:\n",
        "        #error pageObj = pdfReader.getPage(count)\n",
        "        pageObj = pdfReader.pages[count]\n",
        "        count +=1\n",
        "        text += pageObj.extract_text()\n",
        "\n",
        "    if text != \"\":\n",
        "        text = text\n",
        "\n",
        "    else:\n",
        "        text = textract.process('words.txt', method='tesseract', language='eng')\n",
        "\n",
        "\n",
        "    text = text.encode('ascii','ignore').lower()\n",
        "    text_decoded = text.decode()\n",
        "    return text_decoded\n",
        "\n",
        "#Pagewise Implimentation\n",
        "\n",
        "\n",
        "def extract_page(file,num=0):\n",
        "    pdf_file = fitz.open(file)\n",
        "    page_texts = []\n",
        "\n",
        "    for page_number in range(len(pdf_file)):\n",
        "        page = pdf_file.load_page(page_number)\n",
        "        page_text = page.get_text()\n",
        "\n",
        "        # Encode the page text as ASCII and convert to lowercase\n",
        "        page_text = page_text.encode('ascii', 'ignore').lower()\n",
        "        page_text_decoded = page_text.decode()\n",
        "        page_texts.append(page_text_decoded)\n",
        "\n",
        "    #page=int(input(\"Enter Page Number: \"))\n",
        "    filepage = \"\"\n",
        "\n",
        "    if page_texts:\n",
        "      filepage = page_texts[num]\n",
        "\n",
        "\n",
        "    return filepage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleanText(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = text.replace(\"/\", \" \")\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"''\", \"\")\n",
        "    text = text.replace(\"  \", \" \")\n",
        "    text = text.replace(\"  \", \" \")\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub('[‘’“”…]', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    return text\n",
        "round1 = lambda x: cleanText(x)\n",
        "\n",
        "#Remove stopwords\n",
        "stopwords_en = stopwords.words('english')\n",
        "additionalStopWords = [\"pg\",\"figure\",\"u\"]    #Remove the words that are present in the title\n",
        "stopwords_en.extend(additionalStopWords)\n",
        "def toggleStopword(newStopword):\n",
        "  if newStopword in stopwords_en:\n",
        "        stopwords_en.remove(newStopword)\n",
        "  else:\n",
        "        stopwords_en.append(newStopword)\n",
        "\n",
        "def removeStopwords(texts):\n",
        "    extract = ''\n",
        "    for word in simple_preprocess(texts):\n",
        "        if word not in stopwords_en:\n",
        "            extract = extract + ' ' + word\n",
        "    return extract\n",
        "\n",
        "#Lemmatize keywords\n",
        "def lemmatizeKeywords(keywords):\n",
        "    keywords_list = keywords.split(' ')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    keywords_lemmatized = []\n",
        "    for keyword in keywords_list:\n",
        "        keywords_lemmatized.append(lemmatizer.lemmatize(keyword))\n",
        "    return ' '.join(keywords_lemmatized)\n",
        "\n",
        "\n",
        "def identifyFrequentKeywords(keywordsFlat):\n",
        "    keywords = keywordsFlat.split(' ')\n",
        "    while '' in keywords:\n",
        "        keywords.remove('')\n",
        "    keywordsFlat = ' '.join(keywords)\n",
        "    keywordCount = len(keywords)\n",
        "\n",
        "    trigrams_keys = []\n",
        "    trigrams_values = []\n",
        "    trigrams = nltk.trigrams(keywords)\n",
        "    frequency = nltk.FreqDist(trigrams)\n",
        "    for key,value in frequency.items():\n",
        "        if value > 1: #value->absolute count\n",
        "            trigrams_keys.append(' '.join(key))\n",
        "            trigrams_values.append(value/keywordCount) #->persisting relative frequency\n",
        "\n",
        "    bigrams_keys = []\n",
        "    bigrams_values = []\n",
        "    bigrams = nltk.bigrams(keywordsFlat.split(' '))\n",
        "    frequency = nltk.FreqDist(bigrams)\n",
        "    for key,value in frequency.items():\n",
        "        if value > 1:\n",
        "            bigrams_keys.append(' '.join(key))\n",
        "            bigrams_values.append(value/keywordCount)\n",
        "\n",
        "    monograms_keys = []\n",
        "    monograms_values = []\n",
        "    frequency = nltk.FreqDist(keywordsFlat.split(' '))\n",
        "    for key,value in frequency.items():\n",
        "        if value > 1:\n",
        "            monograms_keys.append(key)\n",
        "            monograms_values.append(value/keywordCount)\n",
        "\n",
        "    keys_selected = trigrams_keys + bigrams_keys + monograms_keys\n",
        "    keys_selected_values = trigrams_values + bigrams_values + monograms_values\n",
        "    keys_selected = [x for _, x in sorted(zip(keys_selected_values, keys_selected), reverse=True)]\n",
        "    keys_selected_values.sort(reverse=True)\n",
        "\n",
        "    #select top-n keywords\n",
        "    numberOfTopWords = 5\n",
        "    if len(keys_selected) > numberOfTopWords:\n",
        "        keys_selected        = keys_selected[:numberOfTopWords]\n",
        "        keys_selected_values = keys_selected_values[:numberOfTopWords]\n",
        "\n",
        "    return keys_selected, keys_selected_values, keywordCount\n",
        "\n",
        "#mostFrequentKeywords, relativeKeywordFrequency = identifyFrequentKeywords(lemmatizeKeywords(removeStopwords(round1(text(file)))))\n",
        "#print(mostFrequentKeywords)\n",
        "#print(relativeKeywordFrequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def word_count(word, text1):\n",
        "    with open('/content/drive/MyDrive/test6.txt','w+') as f:\n",
        "        #f.write(cleanText(text(file1)))\n",
        "        f.write(text1)\n",
        "    with open('/content/drive/MyDrive/test6.txt') as f:\n",
        "        return ''.join(f).count(word)\n",
        "\n",
        "\n",
        "#print(\"Number of words in the file :\", word_count(input('give me a word')))\n",
        "\n",
        "def word_cooccurrence(word1,word2,file):\n",
        "  page_list=[]\n",
        "  pages=titel(file)\n",
        "  for i in range(pages):\n",
        "    n=extract_page(file,i)\n",
        "    n=n.lower()\n",
        "    if word1.lower() in n:\n",
        "      if word2.lower() in n:\n",
        "        page_list.append(i+1)\n",
        "  return page_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def CSVFreq(file):\n",
        "  pages=titel(file)\n",
        "  CSV=[]\n",
        "  for i in range(pages):\n",
        "      n=extract_page(file,i)\n",
        "      x=n\n",
        "      x=cleanText(x)\n",
        "      x=lemmatizeKeywords(x)                                  #Turn words into their root ( Happily to Happy, faster to fast, etc)\n",
        "      x=removeStopwords(x)                                    #Only to be used if there's any specific words we want to remove\n",
        "      m=identifyFrequentKeywords(x)                           #Produces two element list of lists\n",
        "      a,b,c=m                                                 #alloting each list to a seperate variable\n",
        "      print(i+1)\n",
        "      print(m)\n",
        "      for j in range(len(a)):\n",
        "          CSV.append([i+1,a[j],b[j],c])\n",
        "  return CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "EU\n"
          ]
        }
      ],
      "source": [
        "def SetCountry(country):\n",
        "  drive.mount('/content/drive')\n",
        "  file = open('/content/drive/MyDrive/Policy/'+country+'.pdf','rb')\n",
        "\n",
        "  return file,country;\n",
        "\n",
        "file,country=SetCountry(\"EU\")    #Write Country name, dont forget capitals first letter\n",
        "print(country)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15]\n"
          ]
        }
      ],
      "source": [
        "x=word_cooccurrence(\"Renewable enERgy source\",\"\",file)  #Only use if you want to check the specific pages where 2 given words are coocccuring. Dont worry about capitals.\n",
        "print(x)                                                #If you want to check a single word/phrase, make the second word just \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "(['european', 'final', 'en', 'council', 'committee'], [0.18181818181818182, 0.09090909090909091, 0.09090909090909091, 0.09090909090909091, 0.09090909090909091], 22)\n",
            "2\n",
            "(['energy', 'supply', 'russian', 'russia', 'repowereu'], [0.06976744186046512, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372], 172)\n",
            "3\n",
            "(['energy', 'eu', 'state', 'member state', 'member'], [0.034055727554179564, 0.030959752321981424, 0.021671826625386997, 0.021671826625386997, 0.021671826625386997], 323)\n",
            "4\n",
            "(['energy', 'saving', 'efficiency', 'measure', 'eu'], [0.09121621621621621, 0.030405405405405407, 0.02702702702702703, 0.02027027027027027, 0.016891891891891893], 296)\n",
            "5\n",
            "(['energy', 'gas', 'demand', 'commission', 'state'], [0.03832752613240418, 0.020905923344947737, 0.017421602787456445, 0.017421602787456445, 0.013937282229965157], 287)\n",
            "6\n",
            "(['eu', 'platform', 'energy', 'supply', 'state'], [0.03296703296703297, 0.03021978021978022, 0.03021978021978022, 0.013736263736263736, 0.013736263736263736], 364)\n",
            "7\n",
            "(['solar', 'energy', 'wind', 'heat', 'commission'], [0.040160642570281124, 0.040160642570281124, 0.024096385542168676, 0.020080321285140562, 0.020080321285140562], 249)\n",
            "8\n",
            "(['hydrogen', 'renewable', 'renewable hydrogen', 'million', 'infrastructure'], [0.09090909090909091, 0.03162055335968379, 0.023715415019762844, 0.019762845849802372, 0.019762845849802372], 253)\n",
            "9\n",
            "(['biomethane', 'eu', 'production', 'gas', 'sustainable'], [0.03557312252964427, 0.02766798418972332, 0.023715415019762844, 0.023715415019762844, 0.019762845849802372], 253)\n",
            "10\n",
            "(['renewable', 'hydrogen', 'energy', 'industrial', 'repowereu'], [0.02768166089965398, 0.02422145328719723, 0.02422145328719723, 0.020761245674740483, 0.01730103806228374], 289)\n",
            "11\n",
            "(['energy', 'skill', 'eu', 'pump', 'material'], [0.023809523809523808, 0.02040816326530612, 0.02040816326530612, 0.017006802721088437, 0.017006802721088437], 294)\n",
            "12\n",
            "(['renewable', 'energy', 'state', 'member state', 'member'], [0.039285714285714285, 0.039285714285714285, 0.03214285714285714, 0.03214285714285714, 0.03214285714285714], 280)\n",
            "13\n",
            "(['gas', 'energy', 'year', 'project', 'bcm'], [0.03987730061349693, 0.03067484662576687, 0.024539877300613498, 0.02147239263803681, 0.018404907975460124], 326)\n",
            "14\n",
            "(['gas', 'supply', 'infrastructure', 'project', 'pipeline'], [0.03636363636363636, 0.024242424242424242, 0.021212121212121213, 0.01818181818181818, 0.015151515151515152], 330)\n",
            "15\n",
            "(['energy', 'electricity', 'oil', 'system', 'supply'], [0.03508771929824561, 0.03070175438596491, 0.021929824561403508, 0.017543859649122806, 0.017543859649122806], 228)\n",
            "16\n",
            "([], [], 4)\n",
            "17\n",
            "(['state', 'member state', 'member', 'investment', 'commission'], [0.03355704697986577, 0.03355704697986577, 0.03355704697986577, 0.026845637583892617, 0.026845637583892617], 149)\n",
            "18\n",
            "(['state', 'member state', 'member', 'energy', 'transfer'], [0.02821316614420063, 0.02821316614420063, 0.02821316614420063, 0.025078369905956112, 0.0219435736677116], 319)\n",
            "19\n",
            "(['state', 'project', 'member state', 'member', 'energy'], [0.04833836858006042, 0.027190332326283987, 0.027190332326283987, 0.027190332326283987, 0.027190332326283987], 331)\n",
            "20\n",
            "(['commission', 'state', 'member state', 'member', 'energy'], [0.02214022140221402, 0.01845018450184502, 0.01845018450184502, 0.01845018450184502, 0.01845018450184502], 271)\n",
            "21\n",
            "(['europe', 'energy', 'russia', 'reduce', 'plan'], [0.03225806451612903, 0.026881720430107527, 0.021505376344086023, 0.021505376344086023, 0.021505376344086023], 186)\n",
            "1\n",
            "(['european', 'final', 'en', 'council', 'committee'], [0.18181818181818182, 0.09090909090909091, 0.09090909090909091, 0.09090909090909091, 0.09090909090909091], 22)\n",
            "2\n",
            "(['energy', 'supply', 'russian', 'russia', 'repowereu'], [0.06976744186046512, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372], 172)\n",
            "3\n",
            "(['energy', 'eu', 'state', 'member state', 'member'], [0.03426791277258567, 0.03115264797507788, 0.021806853582554516, 0.021806853582554516, 0.021806853582554516], 321)\n",
            "4\n",
            "(['energy', 'saving', 'efficiency', 'measure', 'eu'], [0.09152542372881356, 0.030508474576271188, 0.02711864406779661, 0.020338983050847456, 0.01694915254237288], 295)\n",
            "5\n",
            "(['energy', 'gas', 'demand', 'commission', 'state'], [0.03859649122807018, 0.021052631578947368, 0.017543859649122806, 0.017543859649122806, 0.014035087719298246], 285)\n",
            "6\n",
            "(['eu', 'platform', 'energy', 'supply', 'state'], [0.03333333333333333, 0.030555555555555555, 0.030555555555555555, 0.013888888888888888, 0.013888888888888888], 360)\n",
            "7\n",
            "(['solar', 'energy', 'wind', 'heat', 'commission'], [0.04032258064516129, 0.04032258064516129, 0.024193548387096774, 0.020161290322580645, 0.020161290322580645], 248)\n",
            "8\n",
            "(['renewable', 'million', 'infrastructure', 'industry', 'tonne'], [0.034782608695652174, 0.021739130434782608, 0.021739130434782608, 0.021739130434782608, 0.017391304347826087], 230)\n",
            "9\n",
            "(['biomethane', 'eu', 'production', 'gas', 'sustainable'], [0.036, 0.028, 0.024, 0.024, 0.02], 250)\n",
            "10\n",
            "(['renewable', 'energy', 'industrial', 'repowereu', 'production'], [0.028368794326241134, 0.024822695035460994, 0.02127659574468085, 0.01773049645390071, 0.01773049645390071], 282)\n",
            "11\n",
            "(['energy', 'skill', 'eu', 'pump', 'material'], [0.023972602739726026, 0.02054794520547945, 0.02054794520547945, 0.017123287671232876, 0.017123287671232876], 292)\n",
            "12\n",
            "(['renewable', 'energy', 'state', 'member state', 'member'], [0.039285714285714285, 0.039285714285714285, 0.03214285714285714, 0.03214285714285714, 0.03214285714285714], 280)\n",
            "13\n",
            "(['gas', 'energy', 'year', 'project', 'bcm'], [0.03987730061349693, 0.03067484662576687, 0.024539877300613498, 0.02147239263803681, 0.018404907975460124], 326)\n",
            "14\n",
            "(['gas', 'supply', 'infrastructure', 'project', 'pipeline'], [0.03669724770642202, 0.024464831804281346, 0.021406727828746176, 0.01834862385321101, 0.01529051987767584], 327)\n",
            "15\n",
            "(['energy', 'electricity', 'oil', 'system', 'supply'], [0.035398230088495575, 0.030973451327433628, 0.022123893805309734, 0.017699115044247787, 0.017699115044247787], 226)\n",
            "16\n",
            "([], [], 4)\n",
            "17\n",
            "(['state', 'member state', 'member', 'investment', 'commission'], [0.03355704697986577, 0.03355704697986577, 0.03355704697986577, 0.026845637583892617, 0.026845637583892617], 149)\n",
            "18\n",
            "(['state', 'member state', 'member', 'energy', 'transfer'], [0.02830188679245283, 0.02830188679245283, 0.02830188679245283, 0.025157232704402517, 0.0220125786163522], 318)\n",
            "19\n",
            "(['state', 'project', 'member state', 'member', 'energy'], [0.04833836858006042, 0.027190332326283987, 0.027190332326283987, 0.027190332326283987, 0.027190332326283987], 331)\n",
            "20\n",
            "(['commission', 'state', 'member state', 'member', 'energy'], [0.022222222222222223, 0.018518518518518517, 0.018518518518518517, 0.018518518518518517, 0.018518518518518517], 270)\n",
            "21\n",
            "(['europe', 'energy', 'russia', 'reduce', 'plan'], [0.03225806451612903, 0.026881720430107527, 0.021505376344086023, 0.021505376344086023, 0.021505376344086023], 186)\n"
          ]
        }
      ],
      "source": [
        "#TODO: Removal of words in title\n",
        "\n",
        "x=CSVFreq(file)\n",
        "with open('/content/drive/MyDrive/Policy/'+country+'_Frequency.csv','w+',newline='') as f:\n",
        "     writer = csv.writer(f)\n",
        "     writer.writerows(x)\n",
        "\n",
        "\n",
        "toggleStopword(country)      #IF YOU ADD A NEW STOPWORD HERE, ADD IT AT THE BOTTOM AS WELL SO THE ARRAY SELF CORRECTS\n",
        "toggleStopword('hydrogen')   #Add Hydrogen to stopwords\n",
        "\n",
        "\n",
        "x=CSVFreq(file)\n",
        "with open('/content/drive/MyDrive/Policy/'+country+'_Frequency(wo-Stopwords).csv','w+',newline='') as f:\n",
        "     writer = csv.writer(f)\n",
        "     writer.writerows(x)\n",
        "\n",
        "toggleStopword('hydrogen')  # Remove Hydrogen from stopwords again for next file\n",
        "toggleStopword(country)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
